{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qmm7s0qROBj",
        "outputId": "dce61acd-b03d-4fa6-f7b5-5c68fc6cb63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WlzngdSRQKo",
        "outputId": "357818a7-d68f-4554-a495-5f9d7de0daa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import seed\n",
        "np.random.seed(101)\n",
        "tf.random.set_seed(101)\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.layers import Dropout, Conv2D, MaxPool2D , Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaZRf1PegMk4",
        "outputId": "cef8418d-097b-481e-82e7-a7751ca8c654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n",
            "16\n"
          ]
        }
      ],
      "source": [
        "train_path = 'gdrive/My Drive/CV_Project/train'\n",
        "valid_path = 'gdrive/My Drive/CV_Project/test'\n",
        "\n",
        "num_train_samples = 800\n",
        "num_val_samples = 160\n",
        "train_batch_size = 10\n",
        "val_batch_size = 10\n",
        "image_size = 224\n",
        "\n",
        "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
        "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
        "\n",
        "print(int(train_steps))\n",
        "print(int(val_steps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7rgcHlzRSYp",
        "outputId": "a3e99409-7075-4e5a-831e-b77aa277152a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 833 images belonging to 4 classes.\n",
            "Found 833 images belonging to 4 classes.\n",
            "Found 160 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "   \n",
        "    preprocessing_function= preprocess_input)\n",
        "\n",
        "\n",
        "train_batches = datagen.flow_from_directory(train_path,\n",
        "                                           target_size=(224, 224),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='categorical',\n",
        "                                            subset='training')\n",
        "\n",
        "valid_batches = datagen.flow_from_directory(train_path,\n",
        "                                            target_size=(224, 224),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='categorical',\n",
        "                                            subset='training')\n",
        "\n",
        "# Note: shuffle=False causes the test dataset to not be shuffled\n",
        "test_batches = datagen.flow_from_directory(valid_path,\n",
        "                                            target_size=(image_size,image_size),\n",
        "                                            batch_size=1,\n",
        "                                            shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRuiKymAit2d",
        "outputId": "f7af13aa-8804-43da-f445-964fb19ed189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 667 images belonging to 4 classes.\n",
            "Found 166 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Create an ImageDataGenerator for preprocessing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "     validation_split=0.2,\n",
        ")\n",
        "\n",
        "# Load and preprocess training images\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Load and preprocess validation images\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWIKlNhRjPWS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Replace 'image_directory' with the name of the extracted folder in Colab\n",
        "directory = 'gdrive/My Drive/CV_Project/train/chives_images'\n",
        "\n",
        "# List of supported image file extensions\n",
        "supported_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"]\n",
        "\n",
        "for file_name in os.listdir(directory):\n",
        "    \n",
        "    # Check if the file has a supported image extension\n",
        "    if any(file_name.lower().endswith(ext) for ext in supported_extensions):\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "\n",
        "        try:\n",
        "            # Attempt to open the image with PIL\n",
        "            with Image.open(file_path) as img:\n",
        "                a=2\n",
        "                # print(f\"{file_name} is a valid image.\")\n",
        "        except IOError:\n",
        "            print(f\"{file_name} is not a valid image or has issues.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9KP9FyyxUmY",
        "outputId": "08a9f5d4-b9a1-4814-89c7-07776f1a9f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "21/21 [==============================] - 135s 7s/step - loss: 1.5532 - accuracy: 0.2624 - val_loss: 1.3732 - val_accuracy: 0.2952\n",
            "Epoch 2/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 1.3577 - accuracy: 0.3313 - val_loss: 1.3287 - val_accuracy: 0.3494\n",
            "Epoch 3/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 1.2996 - accuracy: 0.4123 - val_loss: 1.2667 - val_accuracy: 0.4157\n",
            "Epoch 4/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 1.1883 - accuracy: 0.4813 - val_loss: 1.2212 - val_accuracy: 0.4157\n",
            "Epoch 5/12\n",
            "21/21 [==============================] - 49s 2s/step - loss: 1.1460 - accuracy: 0.4903 - val_loss: 1.3257 - val_accuracy: 0.3614\n",
            "Epoch 6/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 1.0478 - accuracy: 0.5802 - val_loss: 1.3134 - val_accuracy: 0.3916\n",
            "Epoch 7/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.8265 - accuracy: 0.6837 - val_loss: 1.4189 - val_accuracy: 0.4217\n",
            "Epoch 8/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.7037 - accuracy: 0.7376 - val_loss: 1.5105 - val_accuracy: 0.4578\n",
            "Epoch 9/12\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.5492 - accuracy: 0.8051 - val_loss: 1.5403 - val_accuracy: 0.4518\n",
            "Epoch 10/12\n",
            "21/21 [==============================] - 44s 2s/step - loss: 0.3674 - accuracy: 0.8891 - val_loss: 1.5297 - val_accuracy: 0.4639\n",
            "Epoch 11/12\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.2864 - accuracy: 0.9130 - val_loss: 1.9311 - val_accuracy: 0.4337\n",
            "Epoch 12/12\n",
            "21/21 [==============================] - 43s 2s/step - loss: 0.2569 - accuracy: 0.9250 - val_loss: 1.8934 - val_accuracy: 0.3976\n",
            "Found 160 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "new_model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')  # Replace 4 with the number of classes in your dataset\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "new_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=12,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator)\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o3O9NErdG98",
        "outputId": "fb4326f7-a9f0-447a-d8ef-d16d58714f26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-406a334e17e4>:3: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  metrics = new_model.evaluate_generator(test_generator)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.4000000059604645\n"
          ]
        }
      ],
      "source": [
        "new_model.save('herb_classification_model.h5')\n",
        "\n",
        "metrics = new_model.evaluate_generator(test_generator)\n",
        "print('Test accuracy:', metrics[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diU7tDBVeXZv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = load_img(path, target_size=(224, 224))\n",
        "  x = img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  temp=0\n",
        "  x=0\n",
        "  c=0\n",
        "  for i in classes[0]:\n",
        "    if(i>temp):\n",
        "     \n",
        "      temp=i\n",
        "      x=c\n",
        "    c+=1\n",
        "  class_names={}\n",
        "  class_names={0:'Chives',1:'Oregano',2:'Parsley',3:'Thyme'}\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  print(class_names[x])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}